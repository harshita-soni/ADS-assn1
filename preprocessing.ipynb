{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os, ssl\n",
    "\n",
    "# directory path to store data\n",
    "output_relative_dir = './data'\n",
    "\n",
    "# check if it exists as it makedir will raise an error if it does exist\n",
    "if not os.path.exists(output_relative_dir):\n",
    "    os.makedirs(output_relative_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = '2021'\n",
    "MONTHS = range(6,8)\n",
    "URL_TEMPLATE = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_\"#year-month.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin month 06\n",
      "Completed month 06\n",
      "Begin month 07\n",
      "Completed month 07\n"
     ]
    }
   ],
   "source": [
    "tlc_output_dir = output_relative_dir\n",
    "\n",
    "for month in MONTHS:\n",
    "    # 0-fill i.e 1 -> 01, 2 -> 02, etc\n",
    "    month = str(month).zfill(2) \n",
    "    print(f\"Begin month {month}\")\n",
    "    \n",
    "    # generate url\n",
    "    url = f'{URL_TEMPLATE}{YEAR}-{month}.parquet'\n",
    "    # generate output location and filename\n",
    "    output_dir = f\"{tlc_output_dir}/{YEAR}-{month}.parquet\"\n",
    "\n",
    "    if (not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None)):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    # download\n",
    "    urlretrieve(url, output_dir) \n",
    "    \n",
    "    print(f\"Completed month {month}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ADS\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.parquet('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = pd.read_csv(\"./data/taxi_data/taxi+_zone_lookup.csv\")\n",
    "sf = gpd.read_file(\"./data/taxi_data/taxi_zones.shp\")\n",
    "# attribute tute code\n",
    "sf['geometry'] = sf['geometry'].to_crs(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(pd.merge(zones, sf, on='LocationID', how='inner'))\n",
    "# create a JSON \n",
    "geoJSON = gdf[['LocationID', 'geometry']] \\\n",
    "    .drop_duplicates('LocationID').to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['wkt'] = gdf['geometry'].to_wkt()\n",
    "spark_gdf = spark.createDataFrame(\n",
    "    gdf[['Zone', 'LocationID', 'wkt']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import wkt\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "@F.udf(ArrayType(FloatType()))\n",
    "def get_centroids(wkt_geo):\n",
    "    centroid = wkt.loads(wkt_geo).centroid\n",
    "    return centroid.y, centroid.x\n",
    "\n",
    "spark_gdf = spark_gdf.withColumn(\n",
    "    'geometry',\n",
    "    get_centroids(F.col('wkt'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_gdf.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "from datetime import date\n",
    "\n",
    "def extract_date_time(date_str):\n",
    "    \"\"\"\n",
    "    date string is of the format yyyy-mm-dd hh:mm:ss, e.g., 2022-04-01 00:21:13\n",
    "    Follows 24-hr time format.\n",
    "    Return a tuple of the form (time_str, hour_bin, month, date, day, isWeekend)\n",
    "    \"\"\"\n",
    "    date_time = date_str.split()\n",
    "    if len(date_time) != 2:\n",
    "        return (None, None, None)\n",
    "\n",
    "    dateL = list(map(int, date_time[0].split(\"-\")))\n",
    "    timeL = list(map(int, date_time[1].split(\":\")))\n",
    "    \n",
    "    return dateL, timeL\n",
    "    \n",
    "\n",
    "def extract_features(date_str):\n",
    "    dateL, timeL = extract_date_time(date_str)\n",
    "    \n",
    "    time_str = f'{timeL[0]}:{timeL[1]}'\n",
    "    hour_bin = int(timeL[0])\n",
    "\n",
    "    dateV = date(dateL[0], dateL[1], dateL[2])\n",
    "\n",
    "    day_dict = {0: \"Monday\", 1: \"Tuesday\", 2: \"Wednesday\", 3: \"Thursday\", \n",
    "                4: \"Friday\", 5: \"Saturday\", 6: \"Sunday\"}\n",
    "    day = day_dict[dateV.weekday()]\n",
    "    \n",
    "    is_weekend = 0\n",
    "    if day in [\"Saturday\", \"Sunday\"]:\n",
    "        is_weekend = 1\n",
    "\n",
    "    return (time_str, hour_bin, dateV.month, dateV.day, day, is_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_date_time(\"2022-04-01 00:21:13\")\n",
    "# date(2022, 8, 19).weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean rows with invalid values accoring to the data dictionaries\n",
    "sdf_clean = sdf.filter(\n",
    "    (sdf[\"total_amount\"] < 0) |\n",
    "    (sdf[\"VendorID\"] > 2) | \n",
    "    (sdf[\"VendorID\"] < 1) |\n",
    "    (sdf[\"passenger_count\"] < 1) | \n",
    "    (sdf[\"trip_distance\"] <= 0) | \n",
    "    (sdf[\"RatecodeID\"] > 6) | \n",
    "    (sdf[\"RatecodeID\"] < 1) | \n",
    "    (sdf[\"payment_type\"] > 6) | \n",
    "    (sdf[\"payment_type\"] < 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = sdf_clean.sample(0.05, seed=None)\n",
    "small_df = small_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf.filter(F.col('passenger_count') >= 2).count()\n",
    "CORR_COLS = [\"trip_distance\", \"PULocationID\", \"DOLocationID\", \"total_amount\"]\n",
    "sns.heatmap(small_df[CORR_COLS].corr())\n",
    "\n",
    "plt.title('Pearson Correlation Metric')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = small_df \\\n",
    "    .merge(gdf[['LocationID', 'geometry']], left_on='PULocationID', right_on='LocationID') \\\n",
    "    .drop('LocationID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.withColumn(\n",
    "    'column_to',\n",
    "    some_udf(F.col('column_from'))\n",
    ")\n",
    "\n",
    "for field in ('PU', 'DO'):\n",
    "    _field = f'{field}LocationID'\n",
    "    sdf = sdf.withColumn(\n",
    "        field,\n",
    "        F.col(_field).cast('INT')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
